{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6re+Kz8bk1FWWoHgalVQM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pezzan/AI_workshop/blob/main/RAG_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workshop: Calling an LLM deployed in Azure AI Foundry using `AzureOpenAI` (Python, Colab)\n",
        "\n",
        "## What you will learn\n",
        "By the end of this workshop, you will be able to:\n",
        "1. Understand what a *deployment* is in Azure AI Foundry, and why you call the **deployment name** (not the base model name).\n",
        "2. Configure a Colab notebook safely (no secrets hard-coded).\n",
        "3. Call your deployed LLM using the OpenAI Python SDK `AzureOpenAI` client.\n",
        "4. Build a simple \"conversation memory\" mechanism by keeping and resending message history.\n",
        "\n",
        "---\n",
        "\n",
        "## Concept: Foundry deployments and endpoints\n",
        "\n",
        "In Azure AI Foundry, you deploy a model as a **deployment**:\n",
        "- A deployment is an Azure resource with a **deployment name**, model/version, and configuration.\n",
        "- When you call the API, you route to the right deployment by using its **deployment name**.\n",
        "\n",
        "Foundry can expose multiple endpoint styles. For this workshop we use the **Azure OpenAI inference endpoint** (the one that looks like `https://<resource>.openai.azure.com/`), because the `AzureOpenAI` client is built to talk to Azure OpenAI–style endpoints.\n",
        "\n",
        "> Key rule for Azure OpenAI calls: the `model=` parameter is your **deployment name**.  \n",
        "> If you pass the raw model name (e.g., \"gpt-4o\"), you will typically get an error unless your deployment name happens to match it.\n"
      ],
      "metadata": {
        "id": "vZOHi5J0XEp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install --upgrade openai azure-identity azure-identity-broker"
      ],
      "metadata": {
        "id": "wK6d3LA6RRXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup: environment variables (safe in workshops)\n",
        "\n",
        "We will NOT hard-code keys in the notebook.\n",
        "\n",
        "You need four values from Azure AI Foundry / Azure OpenAI:\n",
        "\n",
        "1. **AZURE_OPENAI_ENDPOINT**  \n",
        "   Looks like: `https://YOUR_RESOURCE_NAME.openai.azure.com/`\n",
        "\n",
        "2. **AZURE_OPENAI_API_KEY**  \n",
        "   A secret key — we will paste it into a hidden input prompt.\n",
        "\n",
        "3. **AZURE_OPENAI_DEPLOYMENT**  \n",
        "   The **deployment name** you created in Foundry (this is what you pass as `model=` in the code).  \n",
        "   Azure requires deployment names even when the parameter is called `model`. :contentReference[oaicite:1]{index=1}\n",
        "\n",
        "4. **AZURE_OPENAI_API_VERSION** (classic API style)  \n",
        "   In many Azure OpenAI examples with `AzureOpenAI`, you specify an `api_version` string (for example `2024-02-01`).  \n",
        "   Your organization might standardize on a specific version; if so, use that."
      ],
      "metadata": {
        "id": "7P_2pjvHGGAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.identity import DefaultAzureCredential"
      ],
      "metadata": {
        "id": "xdaXa2FMNB_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if not os.getenv(\"AZURE_CLIENT_SECRET\"):\n",
        "    os.environ[\"AZURE_CLIENT_SECRET\"] = getpass(\"AZURE_CLIENT_SECRET: \")\n",
        "\n",
        "os.environ[\"AZURE_TENANT_ID\"] = \"0f9e35db-544f-4f60-bdcc-5ea416e6dc70\"\n",
        "os.environ[\"AZURE_CLIENT_ID\"] = \"c473f6c5-2b24-44e5-b44f-b8897e21cbe5\"\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://unepazcdoopenaiprod01.openai.azure.com/\"\n",
        "os.environ[\"AZURE_OPENAI_DEPLOYMENT\"] = \"gpt-4o-mini\"\n",
        "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-12-01-preview\"\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "print(\"Configured:\")\n",
        "print(\"  AZURE_OPENAI_ENDPOINT    =\", os.environ[\"AZURE_OPENAI_ENDPOINT\"])\n",
        "print(\"  AZURE_OPENAI_DEPLOYMENT  =\", os.environ[\"AZURE_OPENAI_DEPLOYMENT\"])\n",
        "print(\"  AZURE_OPENAI_API_VERSION =\", os.environ[\"AZURE_OPENAI_API_VERSION\"])\n"
      ],
      "metadata": {
        "id": "JhvWBnKaGDzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a client\n",
        "\n",
        "The OpenAI Python SDK provides `AzureOpenAI` for Azure OpenAI–style endpoints.\n",
        "\n",
        "We'll pass:\n",
        "- `azure_endpoint`: your endpoint base URL\n",
        "- `api_key`: your Azure OpenAI key\n",
        "- `api_version`: an Azure API version string (classic style)\n",
        "\n",
        "Reminder:\n",
        "- `model=...` must be your **deployment name** in Azure. :contentReference[oaicite:2]{index=2}\n"
      ],
      "metadata": {
        "id": "Ctbi8HpIH3Ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import AzureOpenAI\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
        "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
        "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
        "    azure_ad_token=credential.get_token(\"https://cognitiveservices.azure.com/.default\").token,\n",
        ")\n",
        "\n",
        "deployment_name = os.environ[\"AZURE_OPENAI_DEPLOYMENT\"]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=deployment_name,  # IMPORTANT: model = deployment name in Azure\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant for a technical workshop.\"},\n",
        "        {\"role\": \"user\", \"content\": \"In 3 bullet points, explain what a model deployment is in Azure AI Foundry.\"},\n",
        "    ],\n",
        "    temperature=0.3,\n",
        "    max_tokens=200,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "id": "fTX9E-FvGVvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A reusable helper\n",
        "\n",
        "When teaching, it helps to show a clean function:\n",
        "\n",
        "- Inputs:\n",
        "  - `system_prompt` (controls assistant behavior)\n",
        "  - `user_prompt` (the user's request)\n",
        "- Output:\n",
        "  - assistant text\n",
        "\n",
        "This keeps the \"SDK ceremony\" separate from the teaching content.\n"
      ],
      "metadata": {
        "id": "RLwLNWfdJmDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_llm(user_prompt: str, system_prompt: str = \"You are a helpful assistant.\") -> str:\n",
        "    resp = client.chat.completions.create(\n",
        "        model=deployment_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "        temperature=0.2,\n",
        "        max_tokens=300,\n",
        "    )\n",
        "    return resp.choices[0].message.content\n",
        "\n",
        "print(ask_llm(\"Write a 2-sentence explanation of what an API endpoint is.\"))\n"
      ],
      "metadata": {
        "id": "uDSAy8jSH-1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key idea: the model is stateless\n",
        "\n",
        "A common beginner misconception is: “the model remembers what I said earlier”.\n",
        "\n",
        "In reality, the API is **stateless**:\n",
        "- The model does not keep your history between calls.\n",
        "- If you want it to “remember”, you must send the relevant history again in the next request.\n",
        "\n",
        "That’s why multi-turn chat is implemented by maintaining a `messages` list and resending it each time.\n",
        "\n",
        "This is exactly how official guidance describes building a conversation loop: you keep a running transcript and send it with each new question, otherwise the model “loses context.” :contentReference[oaicite:3]{index=3}\n"
      ],
      "metadata": {
        "id": "OZnx1yQTKW6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ask_llm(\"My name is Peppe. Remember it.\"))\n",
        "print(ask_llm(\"What is my name?\"))  # likely to fail because we didn't include prior context"
      ],
      "metadata": {
        "id": "ajY2vUZPIF0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Keep answers short.\"}\n",
        "]\n",
        "\n",
        "def chat_with_memory(user_text: str) -> str:\n",
        "    messages.append({\"role\": \"user\", \"content\": user_text})\n",
        "    resp = client.chat.completions.create(\n",
        "        model=deployment_name,\n",
        "        messages=messages,\n",
        "        temperature=0.2,\n",
        "        max_tokens=200,\n",
        "    )\n",
        "    assistant_text = resp.choices[0].message.content\n",
        "    messages.append({\"role\": \"assistant\", \"content\": assistant_text})\n",
        "    return assistant_text\n",
        "\n",
        "print(chat_with_memory(\"My name is Peppe. Remember it.\"))\n",
        "print(chat_with_memory(\"What is my name?\"))  # should succeed much more reliably"
      ],
      "metadata": {
        "id": "Hh6sNFvzKhLU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
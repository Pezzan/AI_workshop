{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNF+vmP7wISpD6vhJYQLDbH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pezzan/AI_workshop/blob/main/02-RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workshop 2: Embeddings + Azure AI Search + RAG (keyless auth with `DefaultAzureCredential`)\n",
        "\n",
        "## What you will build\n",
        "You will build a minimal Retrieval-Augmented Generation (RAG) pipeline:\n",
        "\n",
        "1. **Embed** a user question into a vector using an embedding model deployed in Azure AI Foundry.\n",
        "2. **Retrieve** relevant passages from **Azure AI Search** using vector search (and optionally hybrid search).\n",
        "3. **Generate** an answer with an LLM deployment using the retrieved passages as context.\n",
        "\n",
        "---\n",
        "\n",
        "## Why keyless auth?\n",
        "Instead of putting API keys in notebooks, we authenticate with **Microsoft Entra ID** using `DefaultAzureCredential`:\n",
        "\n",
        "- `DefaultAzureCredential` automatically tries multiple credential sources (environment, Azure CLI login, managed identity, etc.). :contentReference[oaicite:0]{index=0}\n",
        "- For Azure OpenAI-style endpoints, the OpenAI Python SDK supports Entra auth via `azure_ad_token_provider` created with `get_bearer_token_provider`. :contentReference[oaicite:1]{index=1}\n",
        "- For Azure AI Search, Microsoft docs show how to build keyless apps using `DefaultAzureCredential` once RBAC is enabled. :contentReference[oaicite:2]{index=2}\n",
        "\n",
        "---\n",
        "\n",
        "## RBAC prerequisites (important)\n",
        "Your identity (user/service principal/managed identity) needs roles:\n",
        "\n",
        "### Azure OpenAI / Foundry deployment\n",
        "Assign **Cognitive Services OpenAI User** on the Azure OpenAI resource (or appropriate scope). :contentReference[oaicite:3]{index=3}\n",
        "\n",
        "### Azure AI Search\n",
        "Enable role-based access on the search service and assign roles such as **Search Index Data Reader** (for querying). :contentReference[oaicite:4]{index=4}\n"
      ],
      "metadata": {
        "id": "2NpZGLvPTdrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install --upgrade openai azure-search-documents azure-core azure-identity azure-identity-broker"
      ],
      "metadata": {
        "id": "f-siOGCuTfSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup: configuration variables (no API keys)\n",
        "\n",
        "You need **two** services configured:\n",
        "\n",
        "### A) Azure AI Foundry / Azure OpenAI-style endpoint (for embeddings + chat)\n",
        "You need:\n",
        "- `AZURE_OPENAI_ENDPOINT`  \n",
        "  Example: `https://YOUR_RESOURCE.openai.azure.com/`\n",
        "- `AZURE_OPENAI_API_VERSION`\n",
        "- Two deployment names:\n",
        "  - `AZURE_OPENAI_CHAT_DEPLOYMENT` (your LLM deployment name)\n",
        "  - `AZURE_OPENAI_EMBED_DEPLOYMENT` (your embedding deployment name)\n",
        "\n",
        "Reminder: in Azure OpenAI-style APIs, the parameter is called `model`, but you pass your **deployment name**.\n",
        "\n",
        "### B) Azure AI Search (for retrieval)\n",
        "You need:\n",
        "- `AZURE_SEARCH_ENDPOINT`  \n",
        "  Example: `https://YOUR-SEARCH-SERVICE.search.windows.net`\n",
        "- `AZURE_SEARCH_INDEX_NAME`\n",
        "\n",
        "---\n",
        "\n",
        "## Authentication options in Colab with `DefaultAzureCredential`\n",
        "\n",
        "### Option 1 (recommended for workshops): Azure CLI device login\n",
        "If you can run `az login` in the notebook, `DefaultAzureCredential` can pick up your Azure CLI session. :contentReference[oaicite:5]{index=5}\n",
        "\n",
        "### Option 2: Service principal via environment variables\n",
        "Set these environment variables (we can prompt for them in a secure way):\n",
        "- `AZURE_TENANT_ID`\n",
        "- `AZURE_CLIENT_ID`\n",
        "- `AZURE_CLIENT_SECRET`\n",
        "\n",
        "Then `DefaultAzureCredential` will use `EnvironmentCredential`.\n",
        "\n",
        "> In production on Azure (Functions/VM/AKS), `DefaultAzureCredential` often uses Managed Identity automatically.\n"
      ],
      "metadata": {
        "id": "3r5-v3K2aG0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "if not os.getenv(\"AZURE_CLIENT_SECRET\"):\n",
        "    os.environ[\"AZURE_CLIENT_SECRET\"] = getpass(\"AZURE_CLIENT_SECRET: \")\n",
        "\n",
        "os.environ[\"AZURE_TENANT_ID\"] = \"0f9e35db-544f-4f60-bdcc-5ea416e6dc70\"\n",
        "os.environ[\"AZURE_CLIENT_ID\"] = \"c473f6c5-2b24-44e5-b44f-b8897e21cbe5\"\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://unepazcdoopenaiprod01.openai.azure.com/\"\n",
        "os.environ[\"AZURE_OPENAI_DEPLOYMENT\"] = \"gpt-4o-mini\"\n",
        "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-12-01-preview\"\n",
        "os.environ[\"AZURE_SEARCH_ENDPOINT\"] = \"https://unepazcdoaisearchprod01.search.windows.net\"\n",
        "os.environ[\"AZURE_SEARCH_INDEX_NAME\"] = \"environmentgpt-v2\"\n",
        "os.environ[\"AZURE_OPENAI_EMBED_DEPLOYMENT\"] = \"text-embedding-3-large\"\n",
        "\n",
        "\n",
        "credential = DefaultAzureCredential()\n"
      ],
      "metadata": {
        "id": "zTiMl-l6TnQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create SDK clients (keyless)\n",
        "\n",
        "We will use:\n",
        "- `AzureOpenAI` (OpenAI Python SDK) for:\n",
        "  - embeddings\n",
        "  - chat completions\n",
        "  authenticated via Entra ID tokens using `get_bearer_token_provider`. :contentReference[oaicite:7]{index=7}\n",
        "\n",
        "- `SearchClient` (Azure AI Search Python SDK) for retrieval,\n",
        "  authenticated via `DefaultAzureCredential` after enabling RBAC on the search service. :contentReference[oaicite:8]{index=8}\n"
      ],
      "metadata": {
        "id": "89L2CR0QayDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "from azure.search.documents import SearchClient\n",
        "from azure.core.credentials import AzureKeyCredential  # not used (kept for reference)\n",
        "\n",
        "aoai = AzureOpenAI(\n",
        "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
        "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
        "    azure_ad_token=credential.get_token(\"https://cognitiveservices.azure.com/.default\").token,\n",
        ")\n",
        "\n",
        "search_client = SearchClient(\n",
        "    endpoint=os.environ[\"AZURE_SEARCH_ENDPOINT\"],\n",
        "    index_name=os.environ[\"AZURE_SEARCH_INDEX_NAME\"],\n",
        "    credential=credential\n",
        ")\n",
        "\n",
        "CHAT_DEPLOYMENT  = os.environ[\"AZURE_OPENAI_DEPLOYMENT\"]\n",
        "EMBED_DEPLOYMENT = os.environ[\"AZURE_OPENAI_EMBED_DEPLOYMENT\"]\n"
      ],
      "metadata": {
        "id": "XJCpGOn7acGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Turn text into a vector (embedding)\n",
        "\n",
        "We embed the **query** (user question) so we can compare it to vectors stored in the search index.\n",
        "\n",
        "### What comes back?\n",
        "An embedding request returns a list of floats (a vector).\n",
        "- The vector length (dimensions) depends on the embedding model.\n",
        "- We store document chunk vectors at indexing time.\n",
        "- At query time, we compute the query vector and ask Azure AI Search for nearest neighbors.\n",
        "\n",
        "### Why keyless auth matters here\n",
        "Because we used `DefaultAzureCredential` + `get_bearer_token_provider`, the OpenAI client will acquire and refresh Entra ID tokens automatically. :contentReference[oaicite:10]{index=10}\n"
      ],
      "metadata": {
        "id": "2SPJXhbrcAbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "def embed_text(text: str) -> List[float]:\n",
        "    \"\"\"Create an embedding vector for a single text input.\"\"\"\n",
        "    if not text or not text.strip():\n",
        "        raise ValueError(\"Text must be a non-empty string.\")\n",
        "\n",
        "    resp = aoai.embeddings.create(\n",
        "        model=EMBED_DEPLOYMENT,  # Azure deployment name\n",
        "        input=text\n",
        "    )\n",
        "    return resp.data[0].embedding\n",
        "\n",
        "# Quick sanity test:\n",
        "vec = embed_text(\"Hello embeddings!\")\n",
        "print(\"Vector length:\", len(vec))\n",
        "print(\"First 5 values:\", vec[:5])\n"
      ],
      "metadata": {
        "id": "4aTThIwDb1fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Retrieve relevant chunks from Azure AI Search (vector search)\n",
        "\n",
        "We’ll do **vector search**:\n",
        "1) compute the query embedding\n",
        "2) ask Azure AI Search for the K nearest chunks\n",
        "\n",
        "### Azure AI Search vector querying\n",
        "Azure AI Search supports vector indexes and vector queries.\n",
        "The query itself must be a vector, which is why we embed the question first. :contentReference[oaicite:11]{index=11}\n",
        "\n",
        "### Keyless auth for Search\n",
        "Microsoft docs describe using `DefaultAzureCredential` for keyless (RBAC-based) connections to Azure AI Search. :contentReference[oaicite:12]{index=12}\n",
        "\n",
        "### You must match your index schema\n",
        "This notebook assumes your index has:\n",
        "- `contentVector` as the vector field\n",
        "- `content` as the chunk text field\n",
        "- optional metadata fields like `id`, `title`, `source`\n",
        "\n",
        "If your fields differ, update `VECTOR_FIELD`, `TEXT_FIELD`, and `select=[...]`.\n"
      ],
      "metadata": {
        "id": "IeZZKp_scJqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.search.documents.models import VectorizedQuery\n",
        "\n",
        "VECTOR_FIELD = \"contentVector\"   # <-- change to match your index\n",
        "TEXT_FIELD   = \"content\"         # <-- change to match your index\n",
        "\n",
        "def vector_search(query: str, k: int = 5):\n",
        "    \"\"\"Vector-only search: returns top-k docs by vector similarity.\"\"\"\n",
        "    qv = embed_text(query)\n",
        "\n",
        "    vq = VectorizedQuery(\n",
        "        vector=qv,\n",
        "        k_nearest_neighbors=k,\n",
        "        fields=VECTOR_FIELD\n",
        "    )\n",
        "\n",
        "    results = search_client.search(\n",
        "        search_text=query,\n",
        "        search_fields=['chunk'],\n",
        "        top=k\n",
        "    )\n",
        "\n",
        "    return list(results)\n",
        "\n",
        "hits = vector_search(\"Which is CO2 impact on the planet?\", k=5)\n",
        "for i, h in enumerate(hits, 1):\n",
        "    print(f\"\\n--- Hit #{i} ---\")\n",
        "    print(\"id    :\", h.get(\"id\"))\n",
        "    print(\"title :\", h.get(\"title\"))\n",
        "    print(\"filename:\", h.get(\"filename\"))\n",
        "    print(\"text  :\", (h.get(\"chunk\") or \"\")[:300], \"...\")\n"
      ],
      "metadata": {
        "id": "MRWC90cQcEWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Generate an answer grounded in retrieved chunks\n",
        "\n",
        "We will:\n",
        "- keep a clean helper function for chat calls (`ask_llm`)\n",
        "- build a `rag_answer` function that:\n",
        "  1) retrieves top chunks from Azure AI Search\n",
        "  2) formats them into a context block\n",
        "  3) asks the LLM to answer using ONLY that context\n",
        "\n",
        "### Why do we force “ONLY the provided context”?\n",
        "Because we want the assistant to behave like a tool:\n",
        "- grounded in enterprise documents\n",
        "- transparent about sources (citations)\n",
        "- willing to say “I don’t know” if retrieval didn’t find enough information\n"
      ],
      "metadata": {
        "id": "CNcolZQqe0mG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_llm(messages, temperature: float = 0.2, max_tokens: int = 400) -> str:\n",
        "    resp = aoai.chat.completions.create(\n",
        "        model=CHAT_DEPLOYMENT,  # Azure deployment name\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "    return resp.choices[0].message.content\n",
        "\n",
        "\n",
        "def format_context(hits, max_chars_per_hit: int = 900) -> str:\n",
        "    blocks = []\n",
        "    for h in hits:\n",
        "        doc_id = h.get(\"id\", \"unknown-id\")\n",
        "        title = h.get(\"title\", \"\")\n",
        "        source = h.get(\"filename\", \"\")\n",
        "        text = (h.get(\"chunk\") or \"\")[:max_chars_per_hit]\n",
        "        blocks.append(f\"[doc_id: {doc_id} | title: {title} | source: {source}]\\n{text}\")\n",
        "    return \"\\n\\n---\\n\\n\".join(blocks)\n",
        "\n",
        "\n",
        "def rag_answer(question: str, k: int = 5, use_hybrid: bool = True) -> str:\n",
        "    hits = vector_search(question, k=k)\n",
        "    context = format_context(hits)\n",
        "\n",
        "    system = (\n",
        "        \"You are a helpful assistant. Use ONLY the provided context to answer. \"\n",
        "        \"If the context is insufficient, say you don't have enough information. \"\n",
        "        \"Cite sources by doc_id in square brackets, e.g. [doc_id: 123].\"\n",
        "    )\n",
        "\n",
        "    user = f\"\"\"QUESTION:\n",
        "{question}\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "- Answer based only on CONTEXT.\n",
        "- Add citations in the form [doc_id: ...].\n",
        "\"\"\"\n",
        "\n",
        "    return ask_llm(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user},\n",
        "        ],\n",
        "        temperature=0.2,\n",
        "        max_tokens=450,\n",
        "    )\n",
        "\n",
        "print(rag_answer(\"Which is CO2 impact on the planet?\", k=5))\n"
      ],
      "metadata": {
        "id": "C3DxOUmOeWlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B1hwzYy3e9pX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}